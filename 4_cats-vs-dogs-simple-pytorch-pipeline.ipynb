{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5441,"databundleVersionId":38425,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Simple Dataset reading pipeline\nDataset is already downloaded. Hence this read takes <1 minutes compared to 20+ minutes from huggingface datasets\n\n### IMPORTANT SIZE OF IMAGE (224, 224) or (256,256)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision, torchinfo, torchmetrics\nimport torchvision\nfrom sklearn.model_selection import train_test_split\nimport os, glob, zipfile\nfrom tqdm import tqdm\nfrom PIL import Image\n\ndef DOWNLOAD_DATASETS():\n    zip_files = ['test.zip', 'train.zip']\n\n    for zip_file in zip_files:\n        with zipfile.ZipFile(\"../input/dogs-vs-cats-redux-kernels-edition/{}\".format(zip_file),\"r\") as z:\n            z.extractall(\".\")\n            print(\"{} unzipped\".format(zip_file))\n\n    train_file_names_list = glob.glob(os.path.join(\"../working/train\",'*.jpg'))\n    test_file_names_list  = glob.glob(os.path.join(\"../working/test\", '*.jpg'))\n\n    train_list, val_list  = train_test_split(train_file_names_list, test_size=0.2)\n\n    transformation_list =  torchvision.transforms.Compose([\n            torchvision.transforms.Resize((224, 224)), # IMPORTANT SIZE OF IMAGE (224, 224) or (256,256)\n            torchvision.transforms.ToTensor(),\n        ])\n\n    class Custom_Dataset(torch.utils.data.Dataset):\n        def __init__(self,file_list,transformation_list = None):\n            self.file_list = file_list\n            self.transform = transformation_list\n\n        def __len__(self):\n            self.filelength = len(self.file_list)\n            return self.filelength\n\n        def __getitem__(self,idx):\n            img_path = self.file_list[idx]\n            img = Image.open(img_path)\n            img_transformed = self.transform(img)\n\n            label = img_path.split('/')[-1].split('.')[0]\n            if label == 'dog':\n                label=1\n            elif label == 'cat':\n                label=0\n\n            return img_transformed,label\n\n    train_dataset = Custom_Dataset(train_list, transformation_list)\n    val_dataset   = Custom_Dataset(val_list  , transformation_list)\n\n    train_loader  = torch.utils.data.DataLoader(dataset = train_dataset, batch_size=32, shuffle=True )\n    val_loader    = torch.utils.data.DataLoader(dataset = val_dataset, batch_size=32, shuffle=True)\n    \n    return train_dataset, val_dataset, train_loader, val_loader\n\n\ntraining_dataset, validation_dataset, training_dataloader, validation_dataloader = DOWNLOAD_DATASETS();\nassert next(iter(training_dataloader)) is not None\nassert next(iter(validation_dataloader)) is not None","metadata":{"_uuid":"96aafc3f-9cae-4abf-959b-f4d218b0cff9","_cell_guid":"8cff02e3-9730-4536-a8d7-06c0d6d206c5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T05:00:24.752172Z","iopub.execute_input":"2024-06-08T05:00:24.752794Z","iopub.status.idle":"2024-06-08T05:00:36.610813Z","shell.execute_reply.started":"2024-06-08T05:00:24.752762Z","shell.execute_reply":"2024-06-08T05:00:36.609959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"```python\n# THIS TAKES at least 10 minutes to process vs Pytorch reading is a lot faster\nimport datasets\n\ndataset_from_hg            = datasets.load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", ignore_verifications= True )\ndef transform_datasets(examples):\n    examples[\"image_tensors\"] = []\n    for image in examples['image']:\n        transformed_image = transformations_group(image)\n        examples['image_tensors'].append(transformed_image)\n\n    return examples\n\ndataset_from_hg = dataset_from_hg   .map(transform_datasets  , batched= True)\n```","metadata":{}},{"cell_type":"markdown","source":"# 2. Simple Model Training Pipeline","metadata":{}},{"cell_type":"code","source":"lr      = 0.001 # learning_rate\nepochs  = 10 # How much to train a model\ndevice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef TRAIN_MODEL(model, training_dataloader, validation_dataloader):\n    \n    model.train(mode=True)\n    OPTIMIZER = torch.optim.SGD ( params= model.parameters(), lr= lr ) # Using torch.optimizer algorithm\n    metric    = torchmetrics.Accuracy(task=\"multiclass\", num_classes= 2 ).to(device)\n    \n    for epoch_no in range(epochs):\n        for batch_no, (image_tensors, labels) in enumerate(progress_bar := tqdm(training_dataloader)):\n            \n            x_actual, y_actual = image_tensors.to(device), labels.to(device)\n            \n            y_predicted_LOGITS = model.forward               (x_actual)\n            y_predicted_probs  = nn.functional.softmax       (y_predicted_LOGITS, dim= 1)\n            loss               = nn.functional.cross_entropy (y_predicted_LOGITS, y_actual.to(torch.int64))\n            \n            OPTIMIZER.zero_grad()\n            loss.backward()\n            # dError_dParameters    = torch.autograd.grad( outputs = ERROR_FUNC( y_predicted, y_actual ), inputs = model.parameters())\n            # Parameters of layer 1 are not dependent on any other parameters\n            # Parameters of layer 2 are dependent on layer 1 parameters\n            # Parameters of layer 3 are dependent on layer 2 parameters which are dependent on layer 1 parameters\n            # Finding complicated rate of change of such nested parameters is done automatically when we do loss.backward()\n            OPTIMIZER.step()\n            \"\"\"\n            for (name, weight), gradient in zip(model.named_parameters(), dError_dWeights):\n                weight = weight - gradient * LEARNING_RATE\n                print(f\"Parameters of layer: {name} have these many {torch.count_nonzero(gradient)} updates out of {torch.count(gradient)})\n            \"\"\"\n\n            loss_batch      = loss.item()\n            accuracy_batch  = metric(y_predicted_LOGITS, y_actual)\n            train_acc_epoch = metric.compute() # calculates average accuracy across epoch automatically\n\n            metrics_per_batch = {\n                \"loss_batch\": loss_batch,\n                \"accuracy_running_average\": train_acc_epoch,\n            }\n            progress_bar.set_description(f'batch_no = {batch_no},\\t loss_batch = {loss_batch:0.4f},\\t accuracy_avg = {train_acc_epoch:0.4f}')\n            \n        metric.reset()\n        loss_validation, accuracy_validation = EVALUATE_MODEL(model, validation_dataloader)\n        print(f'epoch_no = {epoch_no}, training_loss = {loss_batch:0.4f}, validation_loss = {loss_validation:0.4f},\\t training_accuracy = {accuracy_batch:0.4f}, validation_accuracy = {accuracy_validation:0.4f}')\n    \n    model.train(mode=False)\n\n\ndef EVALUATE_MODEL(model, validation_dataloader):\n    # EVALUATE MODEL AT END OF EVERY EPOCH\n    model.eval()\n    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes= 2 ).to(device)\n    with torch.no_grad():\n        for batch_no, (image_tensors, labels) in enumerate(validation_dataloader):\n            x_actual, y_actual = image_tensors.to(device), labels.to(device)\n\n            y_predicted_LOGITS = model.forward                 (x_actual)\n            loss               = nn.functional.cross_entropy   (y_predicted_LOGITS, y_actual.to(torch.int64)).item()\n            accuracy_batch     = metric                        (y_predicted_LOGITS, y_actual).item()\n\n        testing_accuracy_avg = metric.compute().item()\n    return loss, testing_accuracy_avg","metadata":{"_uuid":"ad119a64-e8f5-4201-8bce-f9f155b0f03e","_cell_guid":"e2dbe369-27ac-4420-ab3a-56f720b3825b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T05:05:36.990878Z","iopub.execute_input":"2024-06-08T05:05:36.991586Z","iopub.status.idle":"2024-06-08T05:05:37.006308Z","shell.execute_reply.started":"2024-06-08T05:05:36.991544Z","shell.execute_reply":"2024-06-08T05:05:37.005350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model Architecture Experiments","metadata":{}},{"cell_type":"markdown","source":"## Experiment 1","metadata":{}},{"cell_type":"code","source":"feature_extractor = nn.Sequential(\n    # Standard Input Size: 3, 224, 224\n    # Filter: UV Light filter on glasses is looking for UV lights to filter out or Filter in water purification, is looking for impurities\n    nn.Conv2d                           ( in_channels =  3, out_channels = 50, kernel_size = (3,3), padding=\"same\"), \n    nn.ReLU(), nn.MaxPool2d ((2,2), 2),                                           \n    # 112, 112\n    nn.Conv2d                           ( in_channels = 50, out_channels = 50, kernel_size = (3,3), padding=\"same\"),\n    nn.ReLU(), nn.MaxPool2d ((2,2), 2),\n    # 56, 56\n    nn.Conv2d                           ( in_channels = 50, out_channels = 50, kernel_size = (3,3), padding=\"same\"),\n    nn.ReLU(), nn.MaxPool2d ((2,2), 2),\n    # 28, 28\n    nn.Conv2d                           ( in_channels = 50, out_channels = 50, kernel_size = (3,3), padding=\"same\"),\n    nn.ReLU(), nn.MaxPool2d ((2,2), 2),\n    # 14, 14\n    nn.Conv2d                           ( in_channels = 50, out_channels = 512, kernel_size = (3,3), padding=\"same\"),\n    nn.ReLU(), nn.MaxPool2d ((2,2), 2),\n    # Standard Output Size: 512, 7, 7\n    # Total Features = 512 images of 7*7 pixel\n)\n\ndecision_maker = nn.Sequential(\n    nn.Flatten(start_dim=1),\n    nn.Linear(in_features = 512*7*7 , out_features = 50), nn.ReLU(),\n    nn.Linear(in_features = 50      , out_features = 2) # 1 Neuron is for detecting Cat and 2nd Neuron is for detecting Dog\n)\n\nmodel = nn.Sequential(\n  feature_extractor,\n  decision_maker\n)\nmodel = model.to(device)\n\ntest_example = torch.randn((1,3,224,224)).to(device)\nprint(feature_extractor(test_example).shape, model(test_example).shape)","metadata":{"_uuid":"286f8f56-e799-4852-9693-1717f7edca01","_cell_guid":"f1f536f5-f079-4c94-984d-7b56e01f79fd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T05:14:29.514716Z","iopub.execute_input":"2024-06-08T05:14:29.515474Z","iopub.status.idle":"2024-06-08T05:14:29.615566Z","shell.execute_reply.started":"2024-06-08T05:14:29.515439Z","shell.execute_reply":"2024-06-08T05:14:29.614626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torchinfo.summary(model, input_size= (1,3, 224, 224), verbose=1);","metadata":{"execution":{"iopub.status.busy":"2024-06-08T05:14:31.661164Z","iopub.execute_input":"2024-06-08T05:14:31.661515Z","iopub.status.idle":"2024-06-08T05:14:31.673441Z","shell.execute_reply.started":"2024-06-08T05:14:31.661487Z","shell.execute_reply":"2024-06-08T05:14:31.672424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_MODEL (model, training_dataloader, validation_dataloader)","metadata":{"_uuid":"2e1beb6d-2c67-409e-8761-0e5e171fd436","_cell_guid":"b7422417-423b-4e24-bfee-c666cc6de25a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-06-08T05:19:10.002872Z","iopub.execute_input":"2024-06-08T05:19:10.003266Z","iopub.status.idle":"2024-06-08T05:32:39.463661Z","shell.execute_reply.started":"2024-06-08T05:19:10.003235Z","shell.execute_reply":"2024-06-08T05:32:39.461650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Saving the Trained Model - MOST IMPORTANT PART","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'trained_model_parameter_values.pth')\n\n# Load a trained model\n\nmodel.load_state_dict(torch.load(filename))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experiment 2","metadata":{}},{"cell_type":"code","source":"class Cnn(nn.Module):\n    def __init__(self):\n        super(Cnn,self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3,32,kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2)\n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32,64, kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2)\n            )\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(64,128, kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2)\n        )\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(128,256, kernel_size=3, padding=\"same\"),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2)\n        )\n            \n        self.layer5 = nn.Sequential(\n            nn.Conv2d(256,512, kernel_size=3, padding=\"same\"), # 512 Neurons. I want to take 50 neurons from This neural network to be used in a different problem. \n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.MaxPool2d(2,2)\n        )\n        \n        self.feature_extractor = nn.Sequential (\n            self.layer1,\n            self.layer2,\n            self.layer3,\n            self.layer4,\n            self.layer5,\n        )\n        \n        self.fc1 = nn.Linear(512*7*7,50)\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(50,2)\n        self.relu = nn.ReLU()\n        \n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        \n        extracted_features = self.feature_extractor(x)\n        \n        out = out.view(out.size(0),-1)\n        out = self.relu(self.fc1(out))\n        out = self.fc2(out)\n        return out\n\nmodel = Cnn().to(device)\ntorchinfo.summary(model, input_size= (1,3, 224, 224), verbose=1);\n","metadata":{"execution":{"iopub.status.busy":"2024-06-08T05:46:51.314533Z","iopub.execute_input":"2024-06-08T05:46:51.315411Z","iopub.status.idle":"2024-06-08T05:46:51.378864Z","shell.execute_reply.started":"2024-06-08T05:46:51.315378Z","shell.execute_reply":"2024-06-08T05:46:51.377909Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nCnn                                      [1, 2]                    --\n├─Sequential: 1-1                        --                        (recursive)\n│    └─Sequential: 2-1                   [1, 32, 112, 112]         --\n│    │    └─Conv2d: 3-1                  [1, 32, 224, 224]         896\n│    │    └─BatchNorm2d: 3-2             [1, 32, 224, 224]         64\n│    │    └─ReLU: 3-3                    [1, 32, 224, 224]         --\n│    │    └─MaxPool2d: 3-4               [1, 32, 112, 112]         --\n│    └─Sequential: 2-2                   [1, 64, 56, 56]           --\n│    │    └─Conv2d: 3-5                  [1, 64, 112, 112]         18,496\n│    │    └─BatchNorm2d: 3-6             [1, 64, 112, 112]         128\n│    │    └─ReLU: 3-7                    [1, 64, 112, 112]         --\n│    │    └─MaxPool2d: 3-8               [1, 64, 56, 56]           --\n│    └─Sequential: 2-3                   [1, 128, 28, 28]          --\n│    │    └─Conv2d: 3-9                  [1, 128, 56, 56]          73,856\n│    │    └─BatchNorm2d: 3-10            [1, 128, 56, 56]          256\n│    │    └─ReLU: 3-11                   [1, 128, 56, 56]          --\n│    │    └─MaxPool2d: 3-12              [1, 128, 28, 28]          --\n│    └─Sequential: 2-4                   [1, 256, 14, 14]          --\n│    │    └─Conv2d: 3-13                 [1, 256, 28, 28]          295,168\n│    │    └─BatchNorm2d: 3-14            [1, 256, 28, 28]          512\n│    │    └─ReLU: 3-15                   [1, 256, 28, 28]          --\n│    │    └─MaxPool2d: 3-16              [1, 256, 14, 14]          --\n│    └─Sequential: 2-5                   [1, 512, 7, 7]            --\n│    │    └─Conv2d: 3-17                 [1, 512, 14, 14]          1,180,160\n│    │    └─BatchNorm2d: 3-18            [1, 512, 14, 14]          1,024\n│    │    └─ReLU: 3-19                   [1, 512, 14, 14]          --\n│    │    └─MaxPool2d: 3-20              [1, 512, 7, 7]            --\n├─Sequential: 1-2                        [1, 512, 7, 7]            1,570,560\n│    └─Sequential: 2-6                   [1, 32, 112, 112]         (recursive)\n│    │    └─Conv2d: 3-21                 [1, 32, 224, 224]         (recursive)\n│    │    └─BatchNorm2d: 3-22            [1, 32, 224, 224]         (recursive)\n│    │    └─ReLU: 3-23                   [1, 32, 224, 224]         --\n│    │    └─MaxPool2d: 3-24              [1, 32, 112, 112]         --\n│    └─Sequential: 2-7                   [1, 64, 56, 56]           (recursive)\n│    │    └─Conv2d: 3-25                 [1, 64, 112, 112]         (recursive)\n│    │    └─BatchNorm2d: 3-26            [1, 64, 112, 112]         (recursive)\n│    │    └─ReLU: 3-27                   [1, 64, 112, 112]         --\n│    │    └─MaxPool2d: 3-28              [1, 64, 56, 56]           --\n│    └─Sequential: 2-8                   [1, 128, 28, 28]          (recursive)\n│    │    └─Conv2d: 3-29                 [1, 128, 56, 56]          (recursive)\n│    │    └─BatchNorm2d: 3-30            [1, 128, 56, 56]          (recursive)\n│    │    └─ReLU: 3-31                   [1, 128, 56, 56]          --\n│    │    └─MaxPool2d: 3-32              [1, 128, 28, 28]          --\n│    └─Sequential: 2-9                   [1, 256, 14, 14]          (recursive)\n│    │    └─Conv2d: 3-33                 [1, 256, 28, 28]          (recursive)\n│    │    └─BatchNorm2d: 3-34            [1, 256, 28, 28]          (recursive)\n│    │    └─ReLU: 3-35                   [1, 256, 28, 28]          --\n│    │    └─MaxPool2d: 3-36              [1, 256, 14, 14]          --\n│    └─Sequential: 2-10                  [1, 512, 7, 7]            (recursive)\n│    │    └─Conv2d: 3-37                 [1, 512, 14, 14]          (recursive)\n│    │    └─BatchNorm2d: 3-38            [1, 512, 14, 14]          (recursive)\n│    │    └─ReLU: 3-39                   [1, 512, 14, 14]          --\n│    │    └─MaxPool2d: 3-40              [1, 512, 7, 7]            --\n├─Linear: 1-3                            [1, 50]                   1,254,450\n├─ReLU: 1-4                              [1, 50]                   --\n├─Linear: 1-5                            [1, 2]                    102\n==========================================================================================\nTotal params: 4,395,672\nTrainable params: 4,395,672\nNon-trainable params: 0\nTotal mult-adds (G): 1.94\n==========================================================================================\nInput size (MB): 0.60\nForward/backward pass size (MB): 99.55\nParams size (MB): 11.30\nEstimated Total Size (MB): 111.45\n==========================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"test_example = torch.randn((1,3,224,224)).to(device)\nprint(f'Output of Model is {model(test_example).shape}, Internal Feature Map size is {model.feature_extractor(test_example).shape}')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T05:47:57.623968Z","iopub.execute_input":"2024-06-08T05:47:57.624357Z","iopub.status.idle":"2024-06-08T05:47:57.636457Z","shell.execute_reply.started":"2024-06-08T05:47:57.624326Z","shell.execute_reply":"2024-06-08T05:47:57.635533Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Output of Model is torch.Size([1, 2]), Internal Feature Map size is torch.Size([1, 512, 7, 7])\n","output_type":"stream"}]},{"cell_type":"code","source":"TRAIN_MODEL (model, training_dataloader, validation_dataloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experiment 3 - Work in Progress","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model_2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.conv0 = nn.Conv2d(1,32, 3, padding=1)    # 28 -> 28 | 3\n        \n        # Block 1\n        self.block1 = nn.ModuleDict({\n            \"conv1\": nn.Conv2d(32, 64, 3, padding=1),\n            \"relu1\" : nn.ReLU(),\n            \"conv2\": nn.Conv2d(64, 128, 3, padding=1),\n            \"relu2\" : nn.ReLU(),\n        })\n\n        # Maxpooling before or after 1x1 convolution?\n        self.transition1 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128,32,1), # Squeeze\n        )\n\n        # Block 2\n        self.block2 = nn.ModuleDict({\n            \"conv1\": nn.Conv2d(32, 64, 3, padding=1),\n            \"relu1\" : nn.ReLU(),\n            \"conv2\": nn.Conv2d(64, 128, 3, padding=1),\n            \"relu2\" : nn.ReLU(),\n        })\n        \n        self.transition2 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128,32,1), # Squeeze\n        )\n\n        # Block 3\n        self.block3 = nn.ModuleDict({\n            \"conv1\": nn.Conv2d(32, 64, 3, padding=1),\n            \"relu1\" : nn.ReLU(),\n            \"conv2\": nn.Conv2d(64, 128, 3, padding=1),\n            \"relu2\" : nn.ReLU(),\n        })\n\n        self.transition3 = nn.Sequential(\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(128,32,1), # Squeeze\n        )\n\n        # Block 4\n        self.block4 = nn.ModuleDict({\n            \"conv7\": nn.Conv2d(32, 10, 3),\n        })\n\n    def forward(self, x):\n        b1, b2, b3, b4 = self.block1, self.block2, self.block3, self.block4\n\n        x = self.conv0(x)\n\n        x = b1.relu2(b1.conv2(b1.relu1(b1.conv1(x))))\n        x = self.transition1(x)\n        \n        x = b2.relu2(b2.conv2(b2.relu1(b2.conv1(x))))\n        x = self.transition2(x)\n        \n        x = b3.relu2(b3.conv2(b3.relu1(b3.conv1(x))))\n        x = self.transition3(x)\n\n        x = b4.conv7(x)\n\n        # (-1 = dim 0, 10 = dim 1)\n        x = x.view(-1, 10)\n\n        output = F.log_softmax(x, dim=1)\n        probs = F.softmax(x, dim=1)\n        return output\n","metadata":{},"execution_count":null,"outputs":[]}]}