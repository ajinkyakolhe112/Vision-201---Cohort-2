{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# README\n1. Code is writing in the simplest form possible, to make learning easy. \n2. In just `5 Code Cells`, you will have done your Kaggle Submission for \"Digit Recognizer\" competition \n2. Dataset is downloaded from `Huggingface Datasets`, Model is written in Pytorch, Training Loop is written in pytorch","metadata":{}},{"cell_type":"markdown","source":"# 1. Simple Dataset downloading Pipeline","metadata":{}},{"cell_type":"code","source":"import torch, torch.nn as nn\nimport torchvision, torchinfo, torchmetrics\nimport datasets as huggingface_datasets\nfrom tqdm import tqdm\n\ndevice        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE    = 5\n# IMAGE_RESIZE  = 28,28\n\ndef DOWNLOAD_DATASETS():\n    # Download\n    dataset            = huggingface_datasets.load_dataset(\"mnist\", ) # streaming = True)\n    training_dataset   = dataset['train']\n    validation_dataset = dataset['test']\n\n    # Transform\n    transformations_group = torchvision.transforms.Compose([\n        torchvision.transforms.ToTensor(), # Converts every pixel into value between 0 & 1. \n        # torchvision.transforms.Resize(size=config.IMAGE_RESIZE)\n    ])\n\n    def transform_datasets(examples):\n        examples[\"image_tensors\"] = []\n\n        for image in examples['image']:\n            transformed_image = transformations_group(image)\n            examples['image_tensors'].append(transformed_image)\n\n        return examples\n\n    training_dataset       = training_dataset   .map(transform_datasets  , batched= True)\n    validation_dataset     = validation_dataset .map(transform_datasets  , batched= True)\n\n    # Convert\n    new_training_dataset   = training_dataset   .with_format(\"torch\", columns=['label', 'image_tensors'], dtype = torch.float32)\n    new_validation_dataset = validation_dataset .with_format(\"torch\", columns=['label', 'image_tensors'], dtype = torch.float32)\n\n    TOTAL_BATCHES = len(training_dataset) / BATCH_SIZE\n    \n    training_dataloader   = torch.utils.data.DataLoader( dataset= new_training_dataset   , batch_size= BATCH_SIZE, shuffle= True )\n    validation_dataloader = torch.utils.data.DataLoader( dataset= new_validation_dataset , batch_size= BATCH_SIZE, shuffle= True )\n    \n    return training_dataset, validation_dataset, training_dataloader, validation_dataloader\n\ntraining_dataset, validation_dataset, training_dataloader, validation_dataloader = DOWNLOAD_DATASETS();\nassert next(iter(training_dataloader)) is not None\nassert next(iter(validation_dataloader)) is not None","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-06-07T13:31:00.181391Z","iopub.execute_input":"2024-06-07T13:31:00.182407Z","iopub.status.idle":"2024-06-07T13:32:24.343151Z","shell.execute_reply.started":"2024-06-07T13:31:00.182355Z","shell.execute_reply":"2024-06-07T13:32:24.341579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Simple Model Training Pipeline","metadata":{}},{"cell_type":"code","source":"lr      = 0.001 # learning_rate\nepochs  = 10 # How much to train a model\ndevice  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef TRAIN_MODEL(model, training_dataloader, validation_dataloader):\n\n    model.train(mode=True)\n    OPTIMIZER = torch.optim.SGD ( params= model.parameters(), lr= lr ) # Using torch.optimizer algorithm\n    metric    = torchmetrics.Accuracy(task=\"multiclass\", num_classes= 10 ).to(device)\n    \n    for epoch_no in range(epochs):        \n        for batch_no, batch_dictionary in enumerate(progress_bar := tqdm(training_dataloader)):\n            x_actual = batch_dictionary['image_tensors'].to(device)\n            y_actual = batch_dictionary['label'].to(device)\n\n            y_predicted_LOGITS = model.forward               (x_actual)\n            y_predicted_probs  = nn.functional.softmax       (y_predicted_LOGITS, dim= 1)\n            loss               = nn.functional.cross_entropy (y_predicted_LOGITS, y_actual.to(torch.int64))\n            \n            OPTIMIZER.zero_grad()\n            loss.backward()\n            # dError_dParameters    = torch.autograd.grad( outputs = ERROR_FUNC( y_predicted, y_actual ), inputs = model.parameters())\n            # Parameters of layer 1 are not dependent on any other parameters\n            # Parameters of layer 2 are dependent on layer 1 parameters\n            # Parameters of layer 3 are dependent on layer 2 parameters which are dependent on layer 1 parameters\n            # Finding complicated rate of change of such nested parameters is done automatically when we do loss.backward()\n            OPTIMIZER.step()\n            \"\"\"\n            for (name, weight), gradient in zip(model.named_parameters(), dError_dWeights):\n                weight = weight - gradient * LEARNING_RATE\n                print(f\"Parameters of layer: {name} have these many {torch.count_nonzero(gradient)} updates out of {torch.count(gradient)})\n            \"\"\"\n\n            loss_batch      = loss.item()\n            accuracy_batch  = metric(y_predicted_LOGITS, y_actual)\n            training_accuracy_avg_epoch = metric.compute() # calculates average accuracy across epoch automatically\n\n            metrics_per_batch = {\n                \"loss_batch\": loss_batch,\n                \"accuracy_running_average\": training_accuracy_avg_epoch,\n            }\n            progress_bar.set_description(f'batch_no = {batch_no},\\t loss_batch = {loss_batch:0.4f},\\t accuracy_avg = {training_accuracy_avg_epoch:0.4f}')\n\n        metric.reset()\n        \n        loss_validation, accuracy_validation = EVALUATE_MODEL(model, validation_dataloader)\n        print(f'epoch_no = {epoch_no}, training_loss = {loss_batch:0.4f}, validation_loss = {loss_validation:0.4f},\\t training_accuracy = {accuracy_batch:0.4f}, validation_accuracy = {accuracy_validation:0.4f}')\n        model.train(mode=False)\n\ndef EVALUATE_MODEL(model, validation_dataloader):\n    model.eval()\n    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n    with torch.no_grad():\n        for batch_no, batch_dictionary in enumerate(validation_dataloader):\n            x_actual = batch_dictionary['image_tensors'].to(device)\n            y_actual = batch_dictionary['label'].to(device)\n\n            y_predicted_LOGITS = model.forward                 (x_actual)\n            loss               = nn.functional.cross_entropy   (y_predicted_LOGITS, y_actual.to(torch.int64)).item()\n            accuracy_batch     = metric                        (y_predicted_LOGITS, y_actual).item()\n\n        testing_accuracy_avg = metric.compute().item()\n        return loss, testing_accuracy_avg","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:33:30.915058Z","iopub.execute_input":"2024-06-07T13:33:30.915601Z","iopub.status.idle":"2024-06-07T13:33:30.937479Z","shell.execute_reply.started":"2024-06-07T13:33:30.915561Z","shell.execute_reply":"2024-06-07T13:33:30.935621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Simple Model Architecture","metadata":{}},{"cell_type":"code","source":"model_random_parameters = torch.nn.Sequential(\n    \n    torch.nn.Flatten(start_dim=1),         # Dim:BCHW -> (0:B , 1:C, 2:H, 3:W)\n\n    torch.nn.Linear(in_features = 28*28*1  , out_features   = 40   ), torch.nn.ReLU(),                 # LAYER 1: 1st Hidden Layer\n    torch.nn.Linear(in_features = 40       , out_features = 30   ), torch.nn.ReLU(),                 # LAYER 2: 2nd Hidden Layer\n\n    torch.nn.Linear(in_features = 30       , out_features = 10   ),                                  # OUTPUT LAYER\n)\n\nmodel = model_random_parameters\nmodel = model.to(device)                    # Model Size / Number of Parameters are important\n\ntorchinfo.summary(model, input_size= (1,1*28*28), verbose=2);","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:33:32.827615Z","iopub.execute_input":"2024-06-07T13:33:32.828246Z","iopub.status.idle":"2024-06-07T13:33:32.847116Z","shell.execute_reply.started":"2024-06-07T13:33:32.828173Z","shell.execute_reply":"2024-06-07T13:33:32.845325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Details of Problem Complexity\n- 0.00784 Mega Pixel Image\n- 60,000 of such images\n- They are black and white\n- They contain only numbers 0 to 9\n- Even this SIMPLE PROBLEM, requires a MODEL of MINIMUM 10,000 parameters","metadata":{}},{"cell_type":"code","source":"TRAIN_MODEL (model, training_dataloader, validation_dataloader)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T13:33:38.910791Z","iopub.execute_input":"2024-06-07T13:33:38.911261Z","iopub.status.idle":"2024-06-07T13:34:21.173024Z","shell.execute_reply.started":"2024-06-07T13:33:38.911224Z","shell.execute_reply":"2024-06-07T13:34:21.170955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Kaggle Competition Submission","metadata":{}},{"cell_type":"code","source":"!find /kaggle/input\n\nimport pandas as pd\nimport torch, torchvision\nsubmission_test = pd.read_csv(\"/kaggle/input/digit-recognizer/test.csv\")\n\nx = torch.tensor(submission_test.values.reshape(submission_test.shape[0], 1, 28, 28), dtype=torch.float32)\nx = x.to(device)\n\ndef tensor_to_images(x):\n    images = {}\n    for index in range(x.shape[0]):\n        images[str(index)] = torchvision.transforms.ToPILImage(mode = \"L\" )(x[index])\n    return images\n\nimages = tensor_to_images(x)\n\n# TODO: Figure Out how to do Transforms same way as training Data\ntransformations_list = torchvision.transforms.Compose([\n    torchvision.transforms.ToPILImage(), # Because Data is not saved as Image, we need to first convert it in Image and then convert back to Tensor\n    \n    torchvision.transforms.ToTensor(),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-21T06:12:45.869974Z","iopub.execute_input":"2024-05-21T06:12:45.870812Z","iopub.status.idle":"2024-05-21T06:12:54.920105Z","shell.execute_reply.started":"2024-05-21T06:12:45.870780Z","shell.execute_reply":"2024-05-21T06:12:54.919254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ny_prediction_logits = model(x)\ny_labels_predicted  = torch.argmax(y_prediction_logits, dim = 1)\n\nsubmission          = pd.DataFrame({'ImageId' : torch.arange(1, len(y_labels_predicted) + 1).cpu(), \n                                    'Label'   : y_labels_predicted.cpu()})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T06:18:04.924343Z","iopub.execute_input":"2024-05-20T06:18:04.925334Z","iopub.status.idle":"2024-05-20T06:18:04.972198Z","shell.execute_reply.started":"2024-05-20T06:18:04.925293Z","shell.execute_reply":"2024-05-20T06:18:04.971241Z"},"trusted":true},"execution_count":null,"outputs":[]}]}